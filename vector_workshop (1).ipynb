{"cells":[{"attachments":{},"cell_type":"markdown","id":"a97f5e63-d08e-4edb-b832-b160aaf2be65","metadata":{"language":"python"},"source":"# Initialization\n## Start by installing requried packages"},{"cell_type":"code","execution_count":null,"id":"437b1477-47fe-48ad-96bf-ce85d95fa1fd","metadata":{"language":"python","trusted":true},"outputs":[],"source":"!pip3 install openai --quiet"},{"attachments":{},"cell_type":"markdown","id":"0cb27c72-a4d2-442a-b8b2-3d0e0b22af2f","metadata":{"language":"python"},"source":"## Previewing Data\n\nData should be loaded into a table called `embeddings`. Let's preview that data to see what we're working with."},{"cell_type":"code","execution_count":null,"id":"829c2a23-c77f-493a-be2f-31bb795ca9f6","metadata":{"language":"python","trusted":true},"outputs":[],"source":"%%sql\nselect * from embeddings limit 2;"},{"cell_type":"code","execution_count":13,"id":"f810d1a6-5948-4d7c-9101-1b1be15a4ce9","metadata":{"execution":{"iopub.execute_input":"2024-08-12T20:44:27.816662Z","iopub.status.busy":"2024-08-12T20:44:27.816321Z","iopub.status.idle":"2024-08-12T20:44:27.938694Z","shell.execute_reply":"2024-08-12T20:44:27.938183Z","shell.execute_reply.started":"2024-08-12T20:44:27.816636Z"},"language":"python","trusted":true},"outputs":[{"data":{"text/html":"<table>\n    <thead>\n        <tr>\n            <th>count(*)</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>4403</td>\n        </tr>\n    </tbody>\n</table>","text/plain":"+----------+\n| count(*) |\n+----------+\n|   4403   |\n+----------+"},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":"%%sql\nselect count(*) from embeddings;"},{"attachments":{},"cell_type":"markdown","id":"b0e1f7e8-ed65-4ca5-814e-a443068db6be","metadata":{"language":"python"},"source":"## Building the vector indexes\n\nLet's build our vector index. SingleStore gives us many options for our index with many tunable parameters."},{"attachments":{},"cell_type":"markdown","id":"46c50736-41fc-499e-b53c-a5bda970e03a","metadata":{"language":"python"},"source":"![annvknn](https://github.com/bscolinos/vector_workshop/raw/main/screenshots/knn_v_ann.png)"},{"cell_type":"code","execution_count":null,"id":"40ffb53e-22f3-45bb-b888-f3bbe9048561","metadata":{"language":"python","trusted":true},"outputs":[],"source":"%%sql\nalter table embeddings add vector index auto (v) INDEX_OPTIONS '{\"index_type\":\"AUTO\"}';"},{"attachments":{},"cell_type":"markdown","id":"0cd81cdf-6374-45f8-955a-9b1d4070bc44","metadata":{"language":"python"},"source":"### IVF_PQ\n\nStarting with an IVF_PQ algorithm. Pros: fast build time, low memory usage. Cons: Lower recall than HNSW, slower than HNSW in production"},{"cell_type":"code","execution_count":null,"id":"47d53027-02e3-4927-93dc-a33a522747a5","metadata":{"language":"python","trusted":true},"outputs":[],"source":"%%sql\nalter table embeddings add vector index ivf_pq (v) INDEX_OPTIONS '{\"index_type\":\"IVF_PQ\"}';"},{"attachments":{},"cell_type":"markdown","id":"c4e0b6e9-dacb-4663-9de1-9f55fd573f96","metadata":{"language":"python"},"source":"### HNSW_FLAT\n\nPros: Great recall (similar performance to kNN), great runtime on queries. Cons: Memory intensive. Potentially slow build time"},{"cell_type":"code","execution_count":null,"id":"99837933-c8f7-420f-a4c9-e74dbb873e2d","metadata":{"language":"python","trusted":true},"outputs":[],"source":"%%sql\nalter table embeddings add vector index hnsw_flat (v) INDEX_OPTIONS '{\"index_type\":\"HNSW_FLAT\"}';"},{"attachments":{},"cell_type":"markdown","id":"e8a534be-d834-4db9-bfcd-0183daf38059","metadata":{"language":"python"},"source":"### Syntax to drop indexes if needed"},{"cell_type":"code","execution_count":null,"id":"af88e621-89c2-4fd9-a5cf-9a9f21fc6136","metadata":{"language":"python","trusted":true},"outputs":[],"source":"%%sql\ndrop index auto on embeddings;"},{"attachments":{},"cell_type":"markdown","id":"cca63a5b-a5d4-4ef9-a804-081fd917fcfe","metadata":{"language":"python"},"source":"## Testing our vector indexes\n\nCompare the performance of an exact K-nearest neighbor search to the searches with our ANN indexes.\n\nDue to the relatively small size of the dataset, the search times aren't drastically different, but the increase in speed becomes exponential as the size of the dataset grows."},{"cell_type":"code","execution_count":155,"id":"372befac-5c50-46d0-b129-08d463f8488c","metadata":{"execution":{"iopub.execute_input":"2024-08-12T23:01:12.512736Z","iopub.status.busy":"2024-08-12T23:01:12.512004Z","iopub.status.idle":"2024-08-12T23:01:12.875012Z","shell.execute_reply":"2024-08-12T23:01:12.874263Z","shell.execute_reply.started":"2024-08-12T23:01:12.512703Z"},"language":"python","trusted":true},"outputs":[{"data":{"text/html":"<table>\n    <thead>\n        <tr>\n            <th>content</th>\n            <th>sim</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>ing model predictions that maximized the true positive rate while minimizing<br>the false positive rate across the test set. Each GradCAM heatmap was first<br>converted to a binary mask by thresholding at the lowest non-zero value of the<br>Grad-CAM heatmap. 5 features were used to generate each LIME mask. For ev-<br>ery shape in the image, we calculated the intersection over union (IOU) between<br>the shape and the explanation. Finally, we ranked shapes by their mean IOU<br>across the entire test set.DEPICT: Diffusion-Enabled Permutation Importance 19<br>\"green circle 7<br>(110,16) blue circle<br>23 (44,59) green<br>rectangle ((87, 70),<br>(175, 210)) blue<br>rectangle ((38,<br>104), (55, 183))\"Caption<br> Real<br> Generated<br>\"blue circle 27<br>(227,77) red<br>rectangle ((9, 58),<br>(112, 167)) green<br>rectangle ((126,<br>106), (178, 237))\"<br>\"red rectangle ((123,<br>22), (248, 87))\"<br>Fig.10: Comparison between real and generated images for synthetic<br>dataset. We compare real and generated images from the diffusion model conditioned</td>\n            <td>1.0</td>\n        </tr>\n        <tr>\n            <td>the false positive rate of the validation set. Each GradCAM heatmap was first<br>converted to a binary mask by thresholding at the lowest non-zero value of the<br>Grad-CAM heatmap. 5 features were used to generate each LIME mask. For ev-<br>ery object in the image, we calculated the intersection over union (IOU) between<br>the object mask and the explanation. Finally, we ranked objects by their mean<br>IOU across the entire test set.<br>Unconstrained primary feature models. In reality, we might want to ex-<br>plain a model that is not a concept bottleneck. Thus, we also trained primary<br>feature models end-to-end. When the model is not constrained to a specific set<br>of concepts, we want to observe that DEPICT still detects the primary feature<br>as the most important concept in a classifier’s decisions.<br>9.2 Results<br>Unconstrained primary feature models. We compare three randomly se-<br>lected unconstrained (trained end-to-end) primary feature model rankings gen-</td>\n            <td>0.9171483516693115</td>\n        </tr>\n        <tr>\n            <td>featuremodels that rely on either “person” or “tv” when predicting home or hotel .<br>When permuting the most important concept, model performance is low, whereas when<br>permuting concept that the model does not rely on, model performance does not drop.<br>based explanations, we extend these approaches to generate a ranking by relying<br>on concept annotations and their corresponding mask. Because we have access to<br>the image generation process of the synthetic dataset, we generate an concept-<br>level mask for all concepts in each image. Then, for each image, we calculate the<br>intersection-over-union (IOU) between each concept-level mask and the Grad-<br>CAM or LIME mask generated by the classifier (full details are in supplementary<br>8). Then, we rank concepts by their mean IOU across the entire test set. We note<br>that computing this ranking for GradCAM and LIME requires access to image-<br>level masks as well as the model parameters, while DEPICT does not. Because</td>\n            <td>0.8815749287605286</td>\n        </tr>\n    </tbody>\n</table>","text/plain":"+-------------------------------------------------------------------------------------+--------------------+\n|                                       content                                       |        sim         |\n+-------------------------------------------------------------------------------------+--------------------+\n|     ing model predictions that maximized the true positive rate while minimizing    |        1.0         |\n|     the false positive rate across the test set. Each GradCAM heatmap was first     |                    |\n|    converted to a binary mask by thresholding at the lowest non-zero value of the   |                    |\n|      Grad-CAM heatmap. 5 features were used to generate each LIME mask. For ev-     |                    |\n|   ery shape in the image, we calculated the intersection over union (IOU) between   |                    |\n|      the shape and the explanation. Finally, we ranked shapes by their mean IOU     |                    |\n|    across the entire test set.DEPICT: Diffusion-Enabled Permutation Importance 19   |                    |\n|                                   \"green circle 7                                   |                    |\n|                                 (110,16) blue circle                                |                    |\n|                                   23 (44,59) green                                  |                    |\n|                                 rectangle ((87, 70),                                |                    |\n|                                   (175, 210)) blue                                  |                    |\n|                                   rectangle ((38,                                   |                    |\n|                               104), (55, 183))\"Caption                              |                    |\n|                                         Real                                        |                    |\n|                                       Generated                                     |                    |\n|                                   \"blue circle 27                                   |                    |\n|                                     (227,77) red                                    |                    |\n|                                 rectangle ((9, 58),                                 |                    |\n|                                  (112, 167)) green                                  |                    |\n|                                   rectangle ((126,                                  |                    |\n|                                  106), (178, 237))\"                                 |                    |\n|                                \"red rectangle ((123,                                |                    |\n|                                   22), (248, 87))\"                                  |                    |\n|          Fig.10: Comparison between real and generated images for synthetic         |                    |\n|  dataset. We compare real and generated images from the diffusion model conditioned |                    |\n|    the false positive rate of the validation set. Each GradCAM heatmap was first    | 0.9171483516693115 |\n|    converted to a binary mask by thresholding at the lowest non-zero value of the   |                    |\n|      Grad-CAM heatmap. 5 features were used to generate each LIME mask. For ev-     |                    |\n|   ery object in the image, we calculated the intersection over union (IOU) between  |                    |\n|    the object mask and the explanation. Finally, we ranked objects by their mean    |                    |\n|                           IOU across the entire test set.                           |                    |\n|        Unconstrained primary feature models. In reality, we might want to ex-       |                    |\n|    plain a model that is not a concept bottleneck. Thus, we also trained primary    |                    |\n|    feature models end-to-end. When the model is not constrained to a specific set   |                    |\n|    of concepts, we want to observe that DEPICT still detects the primary feature    |                    |\n|              as the most important concept in a classifier’s decisions.             |                    |\n|                                     9.2 Results                                     |                    |\n|         Unconstrained primary feature models. We compare three randomly se-         |                    |\n|    lected unconstrained (trained end-to-end) primary feature model rankings gen-    |                    |\n|  featuremodels that rely on either “person” or “tv” when predicting home or hotel . | 0.8815749287605286 |\n|  When permuting the most important concept, model performance is low, whereas when  |                    |\n| permuting concept that the model does not rely on, model performance does not drop. |                    |\n|   based explanations, we extend these approaches to generate a ranking by relying   |                    |\n|    on concept annotations and their corresponding mask. Because we have access to   |                    |\n|    the image generation process of the synthetic dataset, we generate an concept-   |                    |\n|  level mask for all concepts in each image. Then, for each image, we calculate the  |                    |\n|     intersection-over-union (IOU) between each concept-level mask and the Grad-     |                    |\n|   CAM or LIME mask generated by the classifier (full details are in supplementary   |                    |\n|   8). Then, we rank concepts by their mean IOU across the entire test set. We note  |                    |\n|      that computing this ranking for GradCAM and LIME requires access to image-     |                    |\n|     level masks as well as the model parameters, while DEPICT does not. Because     |                    |\n+-------------------------------------------------------------------------------------+--------------------+"},"execution_count":155,"metadata":{},"output_type":"execute_result"}],"source":"%%sql\nset @qv = (select v from embeddings where id = 2251799813685522);\n\n-- NO INDEX: exact kNN search\nselect content, v <*> @qv as sim\nfrom embeddings\norder by sim use index () desc\nlimit 3;"},{"cell_type":"code","execution_count":null,"id":"ac2698d1-3dde-4b9b-a7fb-03016429e38c","metadata":{"language":"python","trusted":true},"outputs":[],"source":"%%sql\nset @qv = (select v from embeddings where id = 2251799813685522);\n\n\nselect content, v <*> @qv as sim\nfrom embeddings\norder by sim use index (auto) desc\nlimit 3;"},{"cell_type":"code","execution_count":null,"id":"b2517ef2-6a03-4f0e-8fac-6c0627b7048e","metadata":{"language":"python","trusted":true},"outputs":[],"source":"%%sql\nset @qv = (select v from embeddings where id = 2251799813685522);\n\n-- IVF_PQ\nselect content, v <*> @qv as sim\nfrom embeddings\norder by sim use index (ivf_pq) desc\nlimit 3;"},{"cell_type":"code","execution_count":null,"id":"cac9aa64-628d-4601-bec5-88d50ac5a960","metadata":{"language":"python","trusted":true},"outputs":[],"source":"%%sql\nset @qv = (select v from embeddings where id = 2251799813685522);\n\n-- HNSW_FLAT\nselect content, v <*> @qv as sim\nfrom embeddings\norder by sim use index (hnsw_flat) desc\nlimit 3;"},{"attachments":{},"cell_type":"markdown","id":"b394553f-8dae-416e-a3b3-f061a1a3f6f7","metadata":{"language":"python"},"source":"## Building the fulltext indexes\n\nNow we'll build our fulltext index. As of version 8.7, SingleStore supports Exact phrase search, Fuzzy search, BM25 scoring, Proximity search, Boosting, Wildcard searches, Boolean operators, JSON search via the JLucene library\n\nAdditional information on these can be found here: https://docs.singlestore.com/cloud/developer-resources/functional-extensions/working-with-full-text-search/"},{"cell_type":"code","execution_count":null,"id":"ec8c20e1-ac60-4329-a356-552343315aaf","metadata":{"language":"python","trusted":true},"outputs":[],"source":"%%sql\nalter table embeddings add fulltext using VERSION 2 fts (content);"},{"attachments":{},"cell_type":"markdown","id":"ed028cc0-d2c6-4ec1-924a-4a39e08ba07f","metadata":{"language":"python"},"source":"## Using the fulltext indexes\n\nOnce the fulltext index is built, we can test the various search options. In this example, we'll demonstrate fuzzy matching and a BM25 search"},{"cell_type":"code","execution_count":3,"id":"15687883-1dfb-4737-9d5a-f5e8e2e93678","metadata":{"execution":{"iopub.execute_input":"2024-08-12T20:52:55.655322Z","iopub.status.busy":"2024-08-12T20:52:55.655017Z","iopub.status.idle":"2024-08-12T20:52:56.090065Z","shell.execute_reply":"2024-08-12T20:52:56.089389Z","shell.execute_reply.started":"2024-08-12T20:52:55.655291Z"},"language":"python","trusted":true},"outputs":[{"data":{"text/html":"<table>\n    <thead>\n        <tr>\n            <th>id</th>\n            <th>content</th>\n            <th>score</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>2251799813688675</td>\n            <td>propriate content (e.g., regex for personal identifiers). Fur-<br>thermore, we fine-tune the ELECTRA model (Clark et al.,<br>3Open Artificial Knowledge Dataset<br>2020) on publicly available toxicity datasets (Sorensen et al.,<br>2017) to provide a filtering score.<br>For the Evaluation Metrics (C8) challenge, we will engage<br>the community to fine-tune an LLM on the OAK dataset and<br>evaluate it using common benchmarks such as WinoGrande<br>(Sakaguchi et al., 2021), ARC Easy (Clark et al., 2018), and<br>so forth, the list of the benchmarks will be extended.<br>We plan regular updates of the OAK dataset to reflect new<br>trends and information, addressing the Maintenance and<br>Update of Synthetic Data (C10). This step ensures the<br>dataset remains relevant and effective for training purposes.<br>The data generation pipeline is illustrated in Fig. 1, starting<br>with the querying of knowledge databases to gather topics,<br>which are then expanded using LLMs. These topics are</td>\n            <td>2.03981351852417</td>\n        </tr>\n        <tr>\n            <td>2251799813687603</td>\n            <td>about the extracted code was included, encompassing PL specifications, code complex-<br>ity metrics,  and more  [9]. <br>For this work, version 1.0.7 of the CVEFixes dataset2 was used , which is the most <br>recent version available at the time of writing. Notably, this version incorporates an <br>increased amount of data associated with C/C++ code compared to its predecessors. To <br>use the code within this dataset, it was necessary to extract it from the database. This <br>involved first creating indexes on columns containing identifiers to reduce processing <br>time, as without this optimization, query execution times were too long.  For our analy-<br>sis, the code at function level  was used  along with their respective labels, so o nce the <br>indexes were created, the query  presented in Fig. 1  was executed  to extract the relevant <br>data from the dataset . <br> <br>Fig. 1. SQL query used to extract data from the dataset</td>\n            <td>2.0000267028808594</td>\n        </tr>\n        <tr>\n            <td>2251799813688698</td>\n            <td>high-quality synthetic dataset, forming the foundation for the subsequent steps in the OAK dataset generation pipeline.<br>The methodology for Subtopic Expansion involves several key steps: Initially, high-level topics are extracted from extensive<br>human knowledge databases such as Wikipedia. This ensures a broad and diverse range of starting points for subtopic<br>8Open Artificial Knowledge Dataset<br># List of interest areas<br>interest_areas = [\"technology\", \"history\", \"art\", \"science\", ...]<br>def generate_prompt(categories, page_counts):<br>interest = choose_random(categories, page_counts)<br>ifrandom_chance(0.09):<br>random_title = fetch_random_wikipedia_title()<br>else :<br>page_titles = search_wikipedia(interest, 5)<br>random_title = choose_random(page_titles)<br>summary = fetch_wikipedia_summary(random_title, 2)<br>analysis_type = choose_random([<br>\"summarize\", \"provide an in-depth analysis of\", \"contrast with another topic in<br>the same field\",</td>\n            <td>1.9422978162765503</td>\n        </tr>\n    </tbody>\n</table>","text/plain":"+------------------+------------------------------------------------------------------------------------------------------------------------------+--------------------+\n|        id        |                                                           content                                                            |       score        |\n+------------------+------------------------------------------------------------------------------------------------------------------------------+--------------------+\n| 2251799813688675 |                                propriate content (e.g., regex for personal identifiers). Fur-                                |  2.03981351852417  |\n|                  |                                   thermore, we fine-tune the ELECTRA model (Clark et al.,                                    |                    |\n|                  |                                              3Open Artificial Knowledge Dataset                                              |                    |\n|                  |                               2020) on publicly available toxicity datasets (Sorensen et al.,                                |                    |\n|                  |                                             2017) to provide a filtering score.                                              |                    |\n|                  |                                  For the Evaluation Metrics (C8) challenge, we will engage                                   |                    |\n|                  |                                   the community to fine-tune an LLM on the OAK dataset and                                   |                    |\n|                  |                                    evaluate it using common benchmarks such as WinoGrande                                    |                    |\n|                  |                                 (Sakaguchi et al., 2021), ARC Easy (Clark et al., 2018), and                                 |                    |\n|                  |                                    so forth, the list of the benchmarks will be extended.                                    |                    |\n|                  |                                  We plan regular updates of the OAK dataset to reflect new                                   |                    |\n|                  |                                    trends and information, addressing the Maintenance and                                    |                    |\n|                  |                                    Update of Synthetic Data (C10). This step ensures the                                     |                    |\n|                  |                                dataset remains relevant and effective for training purposes.                                 |                    |\n|                  |                               The data generation pipeline is illustrated in Fig. 1, starting                                |                    |\n|                  |                                  with the querying of knowledge databases to gather topics,                                  |                    |\n|                  |                                     which are then expanded using LLMs. These topics are                                     |                    |\n| 2251799813687603 |                     about the extracted code was included, encompassing PL specifications, code complex-                     | 2.0000267028808594 |\n|                  |                                                ity metrics,  and more  [9].                                                  |                    |\n|                  |                     For this work, version 1.0.7 of the CVEFixes dataset2 was used , which is the most                       |                    |\n|                  |                   recent version available at the time of writing. Notably, this version incorporates an                     |                    |\n|                  |                    increased amount of data associated with C/C++ code compared to its predecessors. To                      |                    |\n|                  |                  use the code within this dataset, it was necessary to extract it from the database. This                    |                    |\n|                  |                   involved first creating indexes on columns containing identifiers to reduce processing                     |                    |\n|                  |                   time, as without this optimization, query execution times were too long.  For our analy-                   |                    |\n|                  |                 sis, the code at function level  was used  along with their respective labels, so o nce the                  |                    |\n|                  |                 indexes were created, the query  presented in Fig. 1  was executed  to extract the relevant                  |                    |\n|                  |                                                   data from the dataset .                                                    |                    |\n|                  |                                                                                                                              |                    |\n|                  |                                   Fig. 1. SQL query used to extract data from the dataset                                    |                    |\n| 2251799813688698 |   high-quality synthetic dataset, forming the foundation for the subsequent steps in the OAK dataset generation pipeline.    | 1.9422978162765503 |\n|                  | The methodology for Subtopic Expansion involves several key steps: Initially, high-level topics are extracted from extensive |                    |\n|                  |     human knowledge databases such as Wikipedia. This ensures a broad and diverse range of starting points for subtopic      |                    |\n|                  |                                              8Open Artificial Knowledge Dataset                                              |                    |\n|                  |                                                   # List of interest areas                                                   |                    |\n|                  |                              interest_areas = [\"technology\", \"history\", \"art\", \"science\", ...]                               |                    |\n|                  |                                        def generate_prompt(categories, page_counts):                                         |                    |\n|                  |                                      interest = choose_random(categories, page_counts)                                       |                    |\n|                  |                                                    ifrandom_chance(0.09):                                                    |                    |\n|                  |                                        random_title = fetch_random_wikipedia_title()                                         |                    |\n|                  |                                                            else :                                                            |                    |\n|                  |                                         page_titles = search_wikipedia(interest, 5)                                          |                    |\n|                  |                                          random_title = choose_random(page_titles)                                           |                    |\n|                  |                                      summary = fetch_wikipedia_summary(random_title, 2)                                      |                    |\n|                  |                                               analysis_type = choose_random([                                                |                    |\n|                  |                       \"summarize\", \"provide an in-depth analysis of\", \"contrast with another topic in                        |                    |\n|                  |                                                       the same field\",                                                       |                    |\n+------------------+------------------------------------------------------------------------------------------------------------------------------+--------------------+"},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":"%%sql\nSELECT id, content, \n       MATCH(TABLE embeddings) AGAINST('content:singlestor~ content:datbase~') AS score\nFROM embeddings\nWHERE MATCH(TABLE embeddings) AGAINST('content:singlestor~ content:datbase~')\nORDER BY score DESC\nLIMIT 3;"},{"cell_type":"code","execution_count":4,"id":"cffeecfa-fd1a-4d1f-970a-076ca829005b","metadata":{"execution":{"iopub.execute_input":"2024-08-12T20:52:59.684645Z","iopub.status.busy":"2024-08-12T20:52:59.683727Z","iopub.status.idle":"2024-08-12T20:52:59.797808Z","shell.execute_reply":"2024-08-12T20:52:59.796939Z","shell.execute_reply.started":"2024-08-12T20:52:59.684610Z"},"language":"python","trusted":true},"outputs":[{"data":{"text/html":"<table>\n    <thead>\n        <tr>\n            <th>id</th>\n            <th>content</th>\n            <th>score_boosted</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>2251799813686900</td>\n            <td>whether we should include that in the generalization bound, since Ris indeed generated based on Z,<br>corresponding to the true Markov chain: S→Z→R.<br>To clarify, the Markov chain for the training without rationale is: S→Z→R, with additional<br>arrows Z→θunandS→θun, but no arrow from Rtoθun.<br>Intuitively, we should account for this difference by arguing that, conditioned on the preference Z,<br>the learned model θunis conditionally independent of R. However, due to this difference, it seems<br>prudent to reason from first principles.<br>Let’s start by choosing the distributions PandQfor the Donsker-Varadhan variational representation<br>of the KL divergence. We set P=PS,R,Z,θ unandQ=µn⊗Pθun, where µis the distribution for<br>17(S, R, Z ). Then, for any measurable function f, we have:<br>D(P∥Q)≥EP[f(S, R, Z, θ )]−logE(¯D,¯R,¯Z,¯θ)∼Q[ef(¯S,¯R,¯Z,¯θ)]. (7)<br>Now, choose f(S, R, Z, θ ) =λ(ℓD(θ)−ℓµ(θ))for some λ∈R, where ℓD(θ)is the empirical loss</td>\n            <td>2.332</td>\n        </tr>\n        <tr>\n            <td>2251799813686071</td>\n            <td>of applying LLMs to controllable and hybrid planning, optimizing for both accuracy and efficiency.<br>This, in turn, allows System- 1.xto outperform both System- 1and System- 2at a fixed budget. By<br>intelligently allocating more resources to harder sub-goals, System- 1.xis able to save its System- 2<br>budget for cases where it is necessary.<br>2 System- 1.x: Controllable and Hybrid Planning with LLMs<br>We start with the requisite background on planning problems and our setup (§2.1), followed by an<br>overview and a detailed description of our method System- 1.xPlanner in §2.2 and §2.3, respectively.<br>2.1 Background and Problem Setup<br>A planning problem can be modeled as a Markov Decision Process M= (S,A,T,R)defined with<br>a set of states S, a set of actions Aon any given state, a transition function T:S ×A → S defining<br>transitions between pairs of states based on an action, and a reward function R:S →Rassigning</td>\n            <td>2.132</td>\n        </tr>\n        <tr>\n            <td>2251799813689499</td>\n            <td>DKL(Pt∥Q)will shrink to zero.<br>3.2. RLHF with KL penalty under heavy-tailed return<br>distribution<br>We now adapt our result to the case where the policy is a<br>language model and we are training it using RLHF. We are<br>now applying KL divergence over the policies rather than<br>the return distributions. We first formally define the prop-<br>erties of RLHF on language models that cause the result to<br>hold: namely, when when considered as a Markov decision<br>process (MDP), environmental transitions are deterministic<br>and return depends only on the final state reached.<br>Definition: A deterministic-transition MDP with Markovian<br>returns (DMRMDP) is an MDP (S,A, P, R )such that:<br>•The transition function P:S×A → S is deterministic,<br>i.e., for each state s∈ S and action a∈ A, there exists<br>a unique state s′∈ Ssuch that P(s′|s, a) = 1 .<br>In RLHF: the transition is appending the generated<br>token ato the context s.<br>•There is a set of sink states E⊆ S that terminate a</td>\n            <td>2.092</td>\n        </tr>\n    </tbody>\n</table>","text/plain":"+------------------+-------------------------------------------------------------------------------------------------------+---------------+\n|        id        |                                                content                                                | score_boosted |\n+------------------+-------------------------------------------------------------------------------------------------------+---------------+\n| 2251799813686900 |   whether we should include that in the generalization bound, since Ris indeed generated based on Z,  |     2.332     |\n|                  |                             corresponding to the true Markov chain: S→Z→R.                            |               |\n|                  |       To clarify, the Markov chain for the training without rationale is: S→Z→R, with additional      |               |\n|                  |                            arrows Z→θunandS→θun, but no arrow from Rtoθun.                            |               |\n|                  |  Intuitively, we should account for this difference by arguing that, conditioned on the preference Z, |               |\n|                  |   the learned model θunis conditionally independent of R. However, due to this difference, it seems   |               |\n|                  |                                prudent to reason from first principles.                               |               |\n|                  |   Let’s start by choosing the distributions PandQfor the Donsker-Varadhan variational representation  |               |\n|                  |         of the KL divergence. We set P=PS,R,Z,θ unandQ=µn⊗Pθun, where µis the distribution for        |               |\n|                  |                      17(S, R, Z ). Then, for any measurable function f, we have:                      |               |\n|                  |                  D(P∥Q)≥EP[f(S, R, Z, θ )]−logE(¯D,¯R,¯Z,¯θ)∼Q[ef(¯S,¯R,¯Z,¯θ)]. (7)                  |               |\n|                  |        Now, choose f(S, R, Z, θ ) =λ(ℓD(θ)−ℓµ(θ))for some λ∈R, where ℓD(θ)is the empirical loss       |               |\n| 2251799813686071 |   of applying LLMs to controllable and hybrid planning, optimizing for both accuracy and efficiency.  |     2.132     |\n|                  |    This, in turn, allows System- 1.xto outperform both System- 1and System- 2at a fixed budget. By    |               |\n|                  | intelligently allocating more resources to harder sub-goals, System- 1.xis able to save its System- 2 |               |\n|                  |                                budget for cases where it is necessary.                                |               |\n|                  |                       2 System- 1.x: Controllable and Hybrid Planning with LLMs                       |               |\n|                  |    We start with the requisite background on planning problems and our setup (§2.1), followed by an   |               |\n|                  |  overview and a detailed description of our method System- 1.xPlanner in §2.2 and §2.3, respectively. |               |\n|                  |                                    2.1 Background and Problem Setup                                   |               |\n|                  |        A planning problem can be modeled as a Markov Decision Process M= (S,A,T,R)defined with        |               |\n|                  |   a set of states S, a set of actions Aon any given state, a transition function T:S ×A → S defining  |               |\n|                  |     transitions between pairs of states based on an action, and a reward function R:S →Rassigning     |               |\n| 2251799813689499 |                                     DKL(Pt∥Q)will shrink to zero.                                     |     2.092     |\n|                  |                          3.2. RLHF with KL penalty under heavy-tailed return                          |               |\n|                  |                                              distribution                                             |               |\n|                  |                       We now adapt our result to the case where the policy is a                       |               |\n|                  |                        language model and we are training it using RLHF. We are                       |               |\n|                  |                        now applying KL divergence over the policies rather than                       |               |\n|                  |                      the return distributions. We first formally define the prop-                     |               |\n|                  |                       erties of RLHF on language models that cause the result to                      |               |\n|                  |                        hold: namely, when when considered as a Markov decision                        |               |\n|                  |                       process (MDP), environmental transitions are deterministic                      |               |\n|                  |                          and return depends only on the final state reached.                          |               |\n|                  |                       Definition: A deterministic-transition MDP with Markovian                       |               |\n|                  |                           returns (DMRMDP) is an MDP (S,A, P, R )such that:                           |               |\n|                  |                          •The transition function P:S×A → S is deterministic,                         |               |\n|                  |                        i.e., for each state s∈ S and action a∈ A, there exists                        |               |\n|                  |                             a unique state s′∈ Ssuch that P(s′|s, a) = 1 .                            |               |\n|                  |                           In RLHF: the transition is appending the generated                          |               |\n|                  |                                        token ato the context s.                                       |               |\n|                  |                          •There is a set of sink states E⊆ S that terminate a                         |               |\n+------------------+-------------------------------------------------------------------------------------------------------+---------------+"},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":"%%sql\nSELECT id, content, ROUND(0.8*BM25(embeddings,'content:markov') + 0.4*BM25(embeddings,'content:intelligence'),3) AS score_boosted\nFROM embeddings\nORDER BY score_boosted DESC\nLIMIT 3;"},{"attachments":{},"cell_type":"markdown","id":"eb6a7eb5-0ccb-4dc3-a2f4-fd52932759a1","metadata":{"execution":{"iopub.execute_input":"2024-07-22T20:44:15.636072Z","iopub.status.busy":"2024-07-22T20:44:15.635760Z","iopub.status.idle":"2024-07-22T20:44:15.640778Z","shell.execute_reply":"2024-07-22T20:44:15.640265Z","shell.execute_reply.started":"2024-07-22T20:44:15.636053Z"},"language":"python"},"source":"## Hybrid Search in SingleStore\n\nImplementing a \"hybrid search\" in SingleStore. This is going to be a query that combines two powerful tools: a fulltext search and a semantic search"},{"cell_type":"code","execution_count":163,"id":"e50400a7-5f82-4226-b16f-d147e60f5cd1","metadata":{"execution":{"iopub.execute_input":"2024-08-12T23:04:41.488320Z","iopub.status.busy":"2024-08-12T23:04:41.487927Z","iopub.status.idle":"2024-08-12T23:04:41.557561Z","shell.execute_reply":"2024-08-12T23:04:41.556881Z","shell.execute_reply.started":"2024-08-12T23:04:41.488285Z"},"language":"python","trusted":true},"outputs":[{"data":{"text/html":"<table>\n    <thead>\n        <tr>\n            <th>id</th>\n            <th>content</th>\n            <th>hybrid_score</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>2251799813686900</td>\n            <td>whether we should include that in the generalization bound, since Ris indeed generated based on Z,<br>corresponding to the true Markov chain: S→Z→R.<br>To clarify, the Markov chain for the training without rationale is: S→Z→R, with additional<br>arrows Z→θunandS→θun, but no arrow from Rtoθun.<br>Intuitively, we should account for this difference by arguing that, conditioned on the preference Z,<br>the learned model θunis conditionally independent of R. However, due to this difference, it seems<br>prudent to reason from first principles.<br>Let’s start by choosing the distributions PandQfor the Donsker-Varadhan variational representation<br>of the KL divergence. We set P=PS,R,Z,θ unandQ=µn⊗Pθun, where µis the distribution for<br>17(S, R, Z ). Then, for any measurable function f, we have:<br>D(P∥Q)≥EP[f(S, R, Z, θ )]−logE(¯D,¯R,¯Z,¯θ)∼Q[ef(¯S,¯R,¯Z,¯θ)]. (7)<br>Now, choose f(S, R, Z, θ ) =λ(ℓD(θ)−ℓµ(θ))for some λ∈R, where ℓD(θ)is the empirical loss</td>\n            <td>0.819</td>\n        </tr>\n        <tr>\n            <td>2251799813686071</td>\n            <td>of applying LLMs to controllable and hybrid planning, optimizing for both accuracy and efficiency.<br>This, in turn, allows System- 1.xto outperform both System- 1and System- 2at a fixed budget. By<br>intelligently allocating more resources to harder sub-goals, System- 1.xis able to save its System- 2<br>budget for cases where it is necessary.<br>2 System- 1.x: Controllable and Hybrid Planning with LLMs<br>We start with the requisite background on planning problems and our setup (§2.1), followed by an<br>overview and a detailed description of our method System- 1.xPlanner in §2.2 and §2.3, respectively.<br>2.1 Background and Problem Setup<br>A planning problem can be modeled as a Markov Decision Process M= (S,A,T,R)defined with<br>a set of states S, a set of actions Aon any given state, a transition function T:S ×A → S defining<br>transitions between pairs of states based on an action, and a reward function R:S →Rassigning</td>\n            <td>0.787</td>\n        </tr>\n        <tr>\n            <td>2251799813689330</td>\n            <td>probability of transitioning between the different tokens, γtdenotes the prob-<br>ability of transitioning to a <MASK>token, and αt= 1−Kβt−γt. Due to the6 S. Chi et al.<br>Markov property, the probabilities of ztat arbitrary diffusion time step can be<br>derived q(zt|z0) =v⊤(zt)Qtv(z0), where Qt=QtQt−1···Q1. The matrix is<br>constructed such that the <MASK>token always maintains its original state so<br>thatztconverges to <MASK>token with sufficiently large t.<br>Conditional Denoising Process. The conditional denoising process through<br>a neural network pθ. This network predicts the noiseless token z0when provided<br>with a corrupted token and its corresponding condition, such as a language<br>token. For training the network pθ, beyond the denoising objective, the training<br>incorporates the standard variational lower bound objective [50], denoted as<br>Lvlb. The training objective with a coefficient for the denoising loss λis:<br>L=Lvlb+λEzt∼q(zt|z0)[−logpθ(z0|zt, y)], (3)</td>\n            <td>0.785</td>\n        </tr>\n    </tbody>\n</table>","text/plain":"+------------------+-------------------------------------------------------------------------------------------------------+--------------+\n|        id        |                                                content                                                | hybrid_score |\n+------------------+-------------------------------------------------------------------------------------------------------+--------------+\n| 2251799813686900 |   whether we should include that in the generalization bound, since Ris indeed generated based on Z,  |    0.819     |\n|                  |                             corresponding to the true Markov chain: S→Z→R.                            |              |\n|                  |       To clarify, the Markov chain for the training without rationale is: S→Z→R, with additional      |              |\n|                  |                            arrows Z→θunandS→θun, but no arrow from Rtoθun.                            |              |\n|                  |  Intuitively, we should account for this difference by arguing that, conditioned on the preference Z, |              |\n|                  |   the learned model θunis conditionally independent of R. However, due to this difference, it seems   |              |\n|                  |                                prudent to reason from first principles.                               |              |\n|                  |   Let’s start by choosing the distributions PandQfor the Donsker-Varadhan variational representation  |              |\n|                  |         of the KL divergence. We set P=PS,R,Z,θ unandQ=µn⊗Pθun, where µis the distribution for        |              |\n|                  |                      17(S, R, Z ). Then, for any measurable function f, we have:                      |              |\n|                  |                  D(P∥Q)≥EP[f(S, R, Z, θ )]−logE(¯D,¯R,¯Z,¯θ)∼Q[ef(¯S,¯R,¯Z,¯θ)]. (7)                  |              |\n|                  |        Now, choose f(S, R, Z, θ ) =λ(ℓD(θ)−ℓµ(θ))for some λ∈R, where ℓD(θ)is the empirical loss       |              |\n| 2251799813686071 |   of applying LLMs to controllable and hybrid planning, optimizing for both accuracy and efficiency.  |    0.787     |\n|                  |    This, in turn, allows System- 1.xto outperform both System- 1and System- 2at a fixed budget. By    |              |\n|                  | intelligently allocating more resources to harder sub-goals, System- 1.xis able to save its System- 2 |              |\n|                  |                                budget for cases where it is necessary.                                |              |\n|                  |                       2 System- 1.x: Controllable and Hybrid Planning with LLMs                       |              |\n|                  |    We start with the requisite background on planning problems and our setup (§2.1), followed by an   |              |\n|                  |  overview and a detailed description of our method System- 1.xPlanner in §2.2 and §2.3, respectively. |              |\n|                  |                                    2.1 Background and Problem Setup                                   |              |\n|                  |        A planning problem can be modeled as a Markov Decision Process M= (S,A,T,R)defined with        |              |\n|                  |   a set of states S, a set of actions Aon any given state, a transition function T:S ×A → S defining  |              |\n|                  |     transitions between pairs of states based on an action, and a reward function R:S →Rassigning     |              |\n| 2251799813689330 |             probability of transitioning between the different tokens, γtdenotes the prob-            |    0.785     |\n|                  |         ability of transitioning to a <MASK>token, and αt= 1−Kβt−γt. Due to the6 S. Chi et al.        |              |\n|                  |            Markov property, the probabilities of ztat arbitrary diffusion time step can be            |              |\n|                  |                  derived q(zt|z0) =v⊤(zt)Qtv(z0), where Qt=QtQt−1···Q1. The matrix is                 |              |\n|                  |              constructed such that the <MASK>token always maintains its original state so             |              |\n|                  |                       thatztconverges to <MASK>token with sufficiently large t.                       |              |\n|                  |                Conditional Denoising Process. The conditional denoising process through               |              |\n|                  |             a neural network pθ. This network predicts the noiseless token z0when provided            |              |\n|                  |               with a corrupted token and its corresponding condition, such as a language              |              |\n|                  |            token. For training the network pθ, beyond the denoising objective, the training           |              |\n|                  |              incorporates the standard variational lower bound objective [50], denoted as             |              |\n|                  |              Lvlb. The training objective with a coefficient for the denoising loss λis:              |              |\n|                  |                              L=Lvlb+λEzt∼q(zt|z0)[−logpθ(z0|zt, y)], (3)                              |              |\n+------------------+-------------------------------------------------------------------------------------------------------+--------------+"},"execution_count":163,"metadata":{},"output_type":"execute_result"}],"source":"%%sql\nSET @qv = (SELECT v FROM embeddings WHERE id = 2251799813685522);\n\nWITH max_bm25 AS (\n    SELECT MAX(0.8 * BM25(embeddings, 'content:markov') + 0.4 * BM25(embeddings, 'content:intelligence')) AS max_score\n    FROM embeddings\n)\nSELECT \n    e.id,\n    e.content,\n    ROUND(\n        (0.7 * (e.v <*> @qv)) + \n        (0.3 * (0.8 * BM25(e, 'content:markov') + 0.4 * BM25(e, 'content:intelligence')) / max_bm25.max_score),\n    3) AS hybrid_score\nFROM \n    embeddings e\nCROSS JOIN max_bm25\nORDER BY \n    hybrid_score DESC\nLIMIT 3;"},{"attachments":{},"cell_type":"markdown","id":"8e35ccc1-9ef0-42d0-88d7-0a4afc8d714c","metadata":{"language":"python"},"source":"## RAG in 3 functions\n\nWe'll test both open and closed source models to compare responses / usability"},{"attachments":{},"cell_type":"markdown","id":"049f7dde-e9f3-4ebf-bcac-145489891727","metadata":{"language":"python"},"source":"![rag](https://github.com/bscolinos/vector_workshop/raw/main/screenshots/rag_architecture.png)"},{"cell_type":"code","execution_count":null,"id":"0eaf2a54-a554-4984-8308-024fede30020","metadata":{"language":"python","trusted":true},"outputs":[],"source":"import sqlalchemy as sa\nfrom openai import OpenAI\nimport os\nimport json"},{"cell_type":"code","execution_count":null,"id":"6f9e726a-3b66-42df-bb87-c2dd4305d9f6","metadata":{"language":"python","trusted":true},"outputs":[],"source":"from singlestoredb.management import get_secret\n\nOPENAI_API_KEY = os.environ['OPENAI_API_KEY'] = get_secret('OPENAI_API_KEY')\nclient = OpenAI()\nEMBEDDING_MODEL = 'text-embedding-ada-002'\nGPT_MODEL = 'gpt-3.5-turbo'\n\n# SingleStore connection\nengine = sa.create_engine(connection_url)\nconnection = engine.connect()"},{"cell_type":"code","execution_count":null,"id":"65bdfa8d-8ebb-4e3f-8a00-6c5436793d17","metadata":{"language":"python","trusted":true},"outputs":[],"source":"def get_embedding(text, model=EMBEDDING_MODEL):\n    '''Generates the OpenAI embedding from an input `text`.'''\n    if isinstance(text, str):\n        response = client.embeddings.create(input=[text], model=model)\n        return json.dumps(response.data[0].embedding)"},{"cell_type":"code","execution_count":null,"id":"9b0d56c8-05e8-4317-9029-870e4254415f","metadata":{"language":"python","trusted":true},"outputs":[],"source":"def vector_search(query, limit=15):\n    '''Returns a df of the top k matches to the query ordered by similarity.'''\n    query_embedding_vec = get_embedding(query)\n    statement = sa.text(\n        f'''select content, v <*> :query_embedding :> vector(1536) AS similarity\n        from embeddings\n        order by similarity use index () desc\n        limit :limit;'''\n    )\n    results = connection.execute(statement, {\"query_embedding\": query_embedding_vec, \"limit\": limit})\n    results_as_dict = results.fetchall()\n    return results_as_dict"},{"cell_type":"code","execution_count":null,"id":"887b1b07-0b67-4867-abac-3cacc61ae00c","metadata":{"language":"python","trusted":true},"outputs":[],"source":"def rag(query, limit=5, temp=0.1):\n    '''Uses RAG to answer a question from the wiki page'''\n    results = vector_search(query, limit)\n    print(\"Asking Chatbot...\")\n    prompt = f'''Excerpt from the conversation history:\n        {results}\n        Question: {query}\n\n        You are a research assistant. Based on the conversation history, try to provide the most accurate answer to the question.\n        Consider the details mentioned in the conversation history to formulate a response that is as\n        helpful and precise as possible. \n\n        '''\n    response = client.chat.completions.create(\n        model=GPT_MODEL,\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a research assistant who is answering questions about an article.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        temperature=temp\n    )\n    response_message = response.choices[0].message.content\n    \n    return response_message"},{"cell_type":"code","execution_count":null,"id":"274bc425-27a2-409a-8773-50c9ee73667a","metadata":{"language":"python","trusted":true},"outputs":[],"source":"query = \"What is a markov chain?\"\nrag(query)"}],"metadata":{"jupyterlab":{"notebooks":{"version_major":6,"version_minor":4}},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"singlestore_cell_default_language":"python","singlestore_connection":{"connectionID":"76afd33e-839a-4e06-b2f1-1171a1d7bbcf","defaultDatabase":""},"singlestore_row_limit":300},"nbformat":4,"nbformat_minor":5}