{"cells":[{"attachments":{},"cell_type":"markdown","id":"0b4f96f3-1088-41e9-8a0c-36a07998cad0","metadata":{"language":"python"},"source":"# Creating the table\n\n- **`id`**: A `bigint` primary key that auto-increments, ensuring unique document identification.\n\n- **`content`**: A `longtext` column for a chunk of the document's main content.\n\n- **`v`**: A `vector` column that stores a 768-dimension vector. This enables advanced search capabilities and similarity comparisons based on the document's content.\n\n- **`metadata`**: A `JSON` column for flexible storage of additional document attributes (in this example we just use the document name, but you can include other data points as needed)"},{"cell_type":"code","execution_count":11,"id":"70884b48-5105-4dba-b8b9-3a25d2aa1c84","metadata":{"execution":{"iopub.execute_input":"2024-08-12T13:13:59.612890Z","iopub.status.busy":"2024-08-12T13:13:59.612447Z","iopub.status.idle":"2024-08-12T13:13:59.746315Z","shell.execute_reply":"2024-08-12T13:13:59.745659Z","shell.execute_reply.started":"2024-08-12T13:13:59.612852Z"},"language":"python","trusted":true},"outputs":[{"data":{"text/html":"<table>\n    <thead>\n        <tr>\n        </tr>\n    </thead>\n    <tbody>\n    </tbody>\n</table>","text/plain":"++\n||\n++\n++"},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":"%%sql\nDROP TABLE IF EXISTS embeddings;\nCREATE TABLE `embeddings` (\n  `id` bigint NOT NULL AUTO_INCREMENT,\n  `content` longtext CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci,\n  `v` vector(768),\n  `metadata` JSON,\n  PRIMARY KEY (`id`)\n);"},{"cell_type":"code","execution_count":12,"id":"ec5e7790-beb8-4879-a565-b368c5bb35fa","metadata":{"execution":{"iopub.execute_input":"2024-08-12T13:14:02.777845Z","iopub.status.busy":"2024-08-12T13:14:02.777531Z","iopub.status.idle":"2024-08-12T13:15:48.215544Z","shell.execute_reply":"2024-08-12T13:15:48.213902Z","shell.execute_reply.started":"2024-08-12T13:14:02.777820Z"},"language":"python","trusted":true},"outputs":[],"source":"!pip install boto3 sentence_transformers pdfplumber langchain --quiet"},{"cell_type":"code","execution_count":14,"id":"b26db564-e20d-4a55-be1c-0edd56d5429c","metadata":{"execution":{"iopub.execute_input":"2024-08-12T13:16:11.427642Z","iopub.status.busy":"2024-08-12T13:16:11.427204Z","iopub.status.idle":"2024-08-12T13:16:11.431347Z","shell.execute_reply":"2024-08-12T13:16:11.430859Z","shell.execute_reply.started":"2024-08-12T13:16:11.427616Z"},"language":"python","trusted":true},"outputs":[],"source":"import boto3\nimport pdfplumber\nfrom sentence_transformers import SentenceTransformer\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nimport io\nfrom concurrent.futures import ThreadPoolExecutor\nimport singlestoredb as s2\nimport json\nimport numpy as np"},{"cell_type":"code","execution_count":16,"id":"1f777eee-ae20-4f21-a531-6ecc616d0b17","metadata":{"execution":{"iopub.execute_input":"2024-08-12T13:16:16.015176Z","iopub.status.busy":"2024-08-12T13:16:16.014543Z","iopub.status.idle":"2024-08-12T13:16:16.128116Z","shell.execute_reply":"2024-08-12T13:16:16.127284Z","shell.execute_reply.started":"2024-08-12T13:16:16.015142Z"},"language":"python","trusted":true},"outputs":[],"source":"# initialize S3 client\nsession = boto3.Session(\n    aws_access_key_id='x', # replace with your access_key\n    aws_secret_access_key='y', # replace with your secret_access_key\n    aws_session_token='z' # replae with your session_token\n)\n\ns3_client = session.client('s3')\n\n# set bucket name\nbucket_name = 'example-vecs'"},{"cell_type":"code","execution_count":17,"id":"735ba3f0-bc7b-46ea-996e-5589b31bacaf","metadata":{"execution":{"iopub.execute_input":"2024-08-12T13:16:18.390358Z","iopub.status.busy":"2024-08-12T13:16:18.390048Z","iopub.status.idle":"2024-08-12T13:16:18.502109Z","shell.execute_reply":"2024-08-12T13:16:18.501311Z","shell.execute_reply.started":"2024-08-12T13:16:18.390331Z"},"language":"python","trusted":true},"outputs":[],"source":"# List all PDF files in the bucket and save to variable\nresponse = s3_client.list_objects_v2(Bucket=bucket_name)\npdf_keys = [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].endswith('.pdf')]"},{"attachments":{},"cell_type":"markdown","id":"488bff00-9dd2-4389-9da6-f9453dbd07f1","metadata":{"language":"python"},"source":"## Embedding Model\nHere, we define our embedding model. We use an open source model in this example, but you can switch this out for any embedding model of your choice, just make sure to adjust the vector dimension in the table DDL."},{"cell_type":"code","execution_count":null,"id":"a79edb59-2fcf-4d65-96c3-be3faf0c0c5f","metadata":{"language":"python","trusted":true},"outputs":[],"source":"# load a pre-trained model for embeddings\nmodel = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"},{"attachments":{},"cell_type":"markdown","id":"d61c18d8-500e-414f-bca2-feb23f4c2983","metadata":{"language":"python"},"source":"## Chunking Methods\n\nChunking is a preprocessing step for RAG that involves breaking down large documents into smaller, manageable pieces of text. Smaller chunks generally allow for more precise information retrieval but potentially lack broader context. Conversely, larger chunks may provide more comprehensive context but can introduce noise and reduce the specificity of retrieved information. Finding the optimal chunk size often requires experimentation and consideration of factors such as the nature of the content, the end user's knowledge level of the information, and the specific requirements of the app.\n\n### 1. Fixed-Size Chunking\n- **Description**: Divides data into chunks of a predetermined size.\n- **Pros**:\n  - Simple to implement.\n  - Efficient for uniform data.\n- **Cons**:\n  - May split meaningful data units.\n  - Inefficient for variable-length data.\n\n### 2. Semantic Chunking\n- **Description**: Divides data based on semantic boundaries (e.g., sentences or paragraphs).\n- **Pros**:\n  - Preserves meaningful data units.\n  - Useful for natural language processing.\n- **Cons**:\n  - Computationally expensive.\n  - Requires language understanding.\n\n### 3. Overlapping Chunking\n- **Description**: Creates chunks with overlapping sections to ensure continuity.\n- **Pros**:\n  - Maintains context between chunks.\n  - Reduces boundary issues.\n- **Cons**:\n  - Increases data redundancy.\n  - Higher storage and processing requirements.\n\n### 4. Dynamic Chunking\n- **Description**: Adjusts chunk size based on data characteristics or processing needs.\n- **Pros**:\n  - Flexible and adaptive.\n  - Optimizes performance for varying data.\n- **Cons**:\n  - Complex to implement.\n  - Requires real-time analysis.\n\n\nHere we use `RecursiveCharacterTextSplitter` (an overlapping chunking method). This involves recursively splitting text into chunks based on character counts, utilizing both a fixed chunk size and a specified overlap between chunks. This balances the need for manageable chunk sizes with the necessity of maintaining context across chunks, particularly beneficial when processing large text datasets or streams."},{"cell_type":"code","execution_count":null,"id":"011bd848-c786-4a19-b164-5c1fe86d7857","metadata":{"language":"python","trusted":true},"outputs":[],"source":"# initialize text splitter\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=50)"},{"cell_type":"code","execution_count":19,"id":"d8424d1d-549b-44bf-811b-87c7c5674b0d","metadata":{"execution":{"iopub.execute_input":"2024-08-12T13:16:24.625723Z","iopub.status.busy":"2024-08-12T13:16:24.625394Z","iopub.status.idle":"2024-08-12T13:16:24.632806Z","shell.execute_reply":"2024-08-12T13:16:24.632179Z","shell.execute_reply.started":"2024-08-12T13:16:24.625694Z"},"language":"python","trusted":true},"outputs":[],"source":"def process_pdf(pdf_key):\n    \"\"\"\n    Processes a PDF by downloading it from S3, extracting its text, splitting the text into chunks, vectorizing each chunk, \n    and storing the results in SingleStore.\n    \"\"\"\n    connection = None\n    try:\n        # Download PDF from S3\n        pdf_object = s3_client.get_object(Bucket=bucket_name, Key=pdf_key)\n        pdf_content = pdf_object['Body'].read()\n\n        # Extract text from PDF using pdfplumber\n        with pdfplumber.open(io.BytesIO(pdf_content)) as pdf:\n            full_text = ''\n            for page in pdf.pages:\n                page_text = page.extract_text()\n                if page_text:\n                    full_text += page_text.encode('utf-8', errors='replace').decode('utf-8', errors='ignore')\n\n        # Split text into chunks\n        chunks = text_splitter.split_text(full_text)\n\n        # Vectorize each chunk\n        vecs = [model.encode(chunk).tolist() for chunk in chunks]  # Convert to list\n\n        # Connect to the database\n        connection = s2.connect(**db_config)\n        cursor = connection.cursor()\n\n        # Insert each chunk and vector into the database\n        for chunk, vector in zip(chunks, vecs):\n            metadata = {'pdf_key': pdf_key}  # using document name as metadata. Add to this as needed\n            insert_query = \"\"\"\n            INSERT INTO embeddings (content, v, metadata)\n            VALUES (%s, %s, %s)\n            \"\"\"\n            cursor.execute(insert_query, (chunk, json.dumps(vector), json.dumps(metadata)))\n\n        # Commit the transaction\n        connection.commit()\n\n    except Exception as e:\n        print(f\"Error processing {pdf_key}: {e}\")\n    finally:\n        if connection:\n            connection.close()\n\n    return pdf_key, len(chunks)"},{"cell_type":"code","execution_count":20,"id":"2b95030b-e848-4555-af9f-87e8daf95558","metadata":{"execution":{"iopub.execute_input":"2024-08-12T13:16:38.456720Z","iopub.status.busy":"2024-08-12T13:16:38.456187Z","iopub.status.idle":"2024-08-12T13:26:31.838935Z","shell.execute_reply":"2024-08-12T13:26:31.838218Z","shell.execute_reply.started":"2024-08-12T13:16:38.456693Z"},"language":"python","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Processed 1706.03762v7.pdf: 19 chunks\nProcessed Adaptive Uncertainty Quantification for Scenarioba.pdf: 17 chunks\nProcessed AudioInsight Detecting Social Contexts Relevant to.pdf: 24 chunks\nProcessed Catastrophic Goodhart regularizing RLHF with KL di.pdf: 24 chunks\nProcessed ChatQA 2 Bridging the Gap to Proprietary LLMs in L.pdf: 17 chunks\nProcessed CheckEval A Checklistbased Approach for Evaluating.pdf: 18 chunks\nProcessed Coarsegraining bistability with the Martini force .pdf: 15 chunks\nProcessed Combining Gradient Information and Primitive Direc.pdf: 25 chunks\nProcessed Computing ground states of spin2 BoseEinstein cond.pdf: 30 chunks\nProcessed Conformal Thresholded Intervals for Efficient Regr.pdf: 18 chunks\nProcessed Contrastive Learning with Counterfactual Explanati.pdf: 26 chunks\nProcessed DEAL Disentangle and Localize Conceptlevel Explana.pdf: 32 chunks\nProcessed DEPICT DiffusionEnabled Permutation Importance for.pdf: 43 chunks\nProcessed DataCentric Human Preference Optimization with Rat.pdf: 52 chunks\nProcessed DiscoverthenName TaskAgnostic Concept Bottlenecks .pdf: 47 chunks\nProcessed Enhancing Layout Hotspot Detection Efficiency with.pdf: 17 chunks\nProcessed Evaluating the Reliability of SelfExplanations in .pdf: 21 chunks\nProcessed Experimental Determination of Wannier Centers in 1.pdf: 36 chunks\nProcessed Explainable Post hoc Portfolio Management Financia.pdf: 27 chunks\nProcessed Foundation Models for Autonomous Robots in Unstruc.pdf: 66 chunks\nProcessed FuzzTheREST An Intelligent Automated Blackbox REST.pdf: 13 chunks\nProcessed Generalized class group actions on oriented ellipt.pdf: 27 chunks\nProcessed Humanintheloop MGA to generate energy system desig.pdf: 25 chunks\nProcessed Indoor Air Quality Dataset with Activities of Dail.pdf: 33 chunks\nProcessed Internal Consistency and SelfFeedback in Large Lan.pdf: 82 chunks\nProcessed InterpBench SemiSynthetic Transformers for Evaluat.pdf: 34 chunks\nProcessed Investigating the EventShape Methods in Search for.pdf: 41 chunks\nProcessed KoMA Knowledgedriven Multiagent Framework for Auto.pdf: 33 chunks\nProcessed LLMs left right and center Assessing GPTs capabili.pdf: 27 chunks\nProcessed Longhorn State Space Models are Amortized Online L.pdf: 25 chunks\nProcessed M2D2M MultiMotion Generation from Text with Discre.pdf: 36 chunks\nProcessed MLMTCNN for Object Detection and Segmentation in M.pdf: 31 chunks\nProcessed Machine learning meets the CHSH scenario.pdf: 36 chunks\nProcessed Mixture of Experts with Mixture of Precisions for .pdf: 13 chunks\nProcessed Multimodal Misinformation Detection using Large Vi.pdf: 31 chunks\nProcessed Nonlinear Schrödinger Network.pdf: 15 chunks\nProcessed On Pretraining of Multimodal Language Models Custo.pdf: 40 chunks\nProcessed On isosceles orthogonality and some geometric cons.pdf: 16 chunks\nProcessed On sibylproof mechanisms.pdf: 10 chunks\nProcessed On the Impact of PRB Load Uncertainty Forecasting .pdf: 20 chunks\nProcessed Open Artificial Knowledge.pdf: 21 chunks\nProcessed PDTPE Parallel Decoder with Textguided Position En.pdf: 19 chunks\nProcessed PolyFormer Scalable Nodewise Filters via Polynomia.pdf: 34 chunks\nProcessed Pulsar timing array sensitivity to anisotropies in.pdf: 38 chunks\nProcessed RedQAOA Efficient Variational Optimization through.pdf: 42 chunks\nProcessed SCoPE Evaluating LLMs for Software Vulnerability D.pdf: 16 chunks\nProcessed Search for VeryShortBaseline Oscillations of React.pdf: 17 chunks\nProcessed Search for orbital magnetism in the kagome superco.pdf: 38 chunks\nProcessed Signatures of composite dark matter in the Cosmic .pdf: 68 chunks\nProcessed SurvReLU Inherently Interpretable Survival Analysi.pdf: 13 chunks\nProcessed System1x Learning to Balance Fast and Slow Plannin.pdf: 49 chunks\nProcessed T2VCompBench A Comprehensive Benchmark for Composi.pdf: 36 chunks\nProcessed The Extrapolation Power of Implicit Models.pdf: 23 chunks\nProcessed The FAST HI 21cm absorption blind survey II  stati.pdf: 52 chunks\nProcessed The Vision of Autonomic Computing Can LLMs Make It.pdf: 45 chunks\nProcessed Undermining Mental Proof How AI Can Make Cooperati.pdf: 22 chunks\nProcessed Watermark Smoothing Attacks against Language Model.pdf: 24 chunks\nProcessed Words2Contact Identifying Support Contacts from Ve.pdf: 22 chunks\n"}],"source":"with ThreadPoolExecutor(max_workers=4) as executor:\n    futures = {executor.submit(process_pdf, pdf_key): pdf_key for pdf_key in pdf_keys}\n    for future in futures:\n        pdf_key = futures[future]\n        try:\n            key, num_chunks = future.result()\n            print(f\"Processed {key}: {num_chunks} chunks\")\n        except Exception as e:\n            print(f\"Error processing {pdf_key}: {e}\")"},{"cell_type":"code","execution_count":37,"id":"152df5da-74db-4ff3-923e-c494fa028847","metadata":{"execution":{"iopub.execute_input":"2024-08-12T13:29:13.512639Z","iopub.status.busy":"2024-08-12T13:29:13.512232Z","iopub.status.idle":"2024-08-12T13:29:13.646664Z","shell.execute_reply":"2024-08-12T13:29:13.646089Z","shell.execute_reply.started":"2024-08-12T13:29:13.512601Z"},"language":"python","trusted":true},"outputs":[{"data":{"text/html":"<table>\n    <thead>\n        <tr>\n            <th>count(*)</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>1741</td>\n        </tr>\n    </tbody>\n</table>","text/plain":"+----------+\n| count(*) |\n+----------+\n|   1741   |\n+----------+"},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":"%%sql\nselect count(*) from embeddings;"},{"cell_type":"code","execution_count":4,"id":"8da234fd-794c-4ea5-9ea0-f48b6a5f301b","metadata":{"execution":{"iopub.execute_input":"2024-08-12T14:39:54.481297Z","iopub.status.busy":"2024-08-12T14:39:54.480996Z","iopub.status.idle":"2024-08-12T14:39:54.579297Z","shell.execute_reply":"2024-08-12T14:39:54.578518Z","shell.execute_reply.started":"2024-08-12T14:39:54.481259Z"},"language":"python","trusted":true},"outputs":[{"data":{"text/html":"<table>\n    <thead>\n        <tr>\n            <th>metadata</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>{'pdf_key': 'Catastrophic Goodhart regularizing RLHF with KL di.pdf'}</td>\n        </tr>\n        <tr>\n            <td>{'pdf_key': 'AudioInsight Detecting Social Contexts Relevant to.pdf'}</td>\n        </tr>\n        <tr>\n            <td>{'pdf_key': 'Conformal Thresholded Intervals for Efficient Regr.pdf'}</td>\n        </tr>\n        <tr>\n            <td>{'pdf_key': 'DataCentric Human Preference Optimization with Rat.pdf'}</td>\n        </tr>\n        <tr>\n            <td>{'pdf_key': 'Evaluating the Reliability of SelfExplanations in .pdf'}</td>\n        </tr>\n    </tbody>\n</table>","text/plain":"+-----------------------------------------------------------------------+\n|                                metadata                               |\n+-----------------------------------------------------------------------+\n| {'pdf_key': 'Catastrophic Goodhart regularizing RLHF with KL di.pdf'} |\n| {'pdf_key': 'AudioInsight Detecting Social Contexts Relevant to.pdf'} |\n| {'pdf_key': 'Conformal Thresholded Intervals for Efficient Regr.pdf'} |\n| {'pdf_key': 'DataCentric Human Preference Optimization with Rat.pdf'} |\n| {'pdf_key': 'Evaluating the Reliability of SelfExplanations in .pdf'} |\n+-----------------------------------------------------------------------+"},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":"%%sql\nselect metadata from embeddings limit 5;"}],"metadata":{"jupyterlab":{"notebooks":{"version_major":6,"version_minor":4}},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"singlestore_cell_default_language":"python","singlestore_connection":{"connectionID":"76afd33e-839a-4e06-b2f1-1171a1d7bbcf","defaultDatabase":""},"singlestore_row_limit":300},"nbformat":4,"nbformat_minor":5}